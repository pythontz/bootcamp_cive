{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Algebra for Machine Learning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiplying matrices and vectors\n",
    "\n",
    "** Matrices Product:** The matrix product of matrices $\\textbf{A}$ and $\\textbf{B}$ is a third matrix $\\textbf{C}$.\n",
    "\n",
    "** Properties of Matrices products: ** \n",
    "* *Distributive:* $A(B + C) =AB + AC$\n",
    "* *Associative:* $ABC = (AB)C$\n",
    "* The transpose of a matrix product has a simple form $(AB)^T = B^TA$\n",
    "\n",
    "**Note**: Matrix maltiplication is not commutative i.e $AB \\neq BA$\n",
    "\n",
    "\n",
    "** Dot product ** between two vectors $x$ and $y$ of the same dimensionality is the matrix product $\\mathbf{x^Ty}$. The dot product between two vectors is commutative i.e $$\\mathbf{x^T y=y^Tx}$$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** There are two ways**.\n",
    "We can either use the **np.dot** function, which applies a matrix-matrix, matrix-vector, or inner vector multiplication to its two arguments: OR\n",
    "Use multiplication which applies to matrix-matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np # naming import convention\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Let create an array of  A and B\n",
    "A = np.array([[ 1.,  0., 1.],[ -1.,  1., 0.],[1.,  0.,  -1.]])\n",
    "\n",
    "B = np.array([[2,1,-2], [-2,2,1], [1,-2,2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "C = np.dot(A,B)\n",
    "C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Or you can write\n",
    "C = A.dot(B)\n",
    "C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create a vector v\n",
    "v = np.array([5, 2.5, 0.5])\n",
    "np.dot(A, v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "Create two vectors of random values between -10 and 10 of size 100. Compute:\n",
    "\n",
    "* elementwise product between them\n",
    "* dot (scalar) product between them.\n",
    "\n",
    "Compare your results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identity and Inverse Matrices\n",
    "\n",
    "An *identity matrix* is a matrix that does not change any vector when we multiply that vector by that matrix denoted as $I_n \\in  \\mathbb{R}^{n \\times n}$.\n",
    "\n",
    "The matrix inverse of $A$ is denoted as $A^{-1}$, is defined as the matrix such that:\n",
    "$$ \\mathbf{A A^{-1} = I_n} $$\n",
    "\n",
    "**Inverse: np.linalg.inv**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.linalg.inv(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding Determinant\n",
    "\n",
    "The determinant of a square matrix $\\mathbf{A}$ is often denoted $\\mid\\mathbf{A}\\mid$ and is a quantity often used in linear algebra. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.linalg.det(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Equations\n",
    "\n",
    "A system of linear equations is given as $$ \\mathbf{Ax =b}$$ where $A \\in \\mathbb{R}^{m \\times n}$ is a known matrix, $b \\in \\mathbb{R}^{m}$ is a known vector, and $x \\in \\mathbb{R}^n$ is a\n",
    "vector of unknown variables. We can solve for $\\mathbf{x}$ by following steps:\n",
    "\n",
    "\\begin{align*}\n",
    "Ax &=b \\\\\n",
    "A^{-1} Ax &= A^{-1} b \\\\\n",
    "I_n x &=A^{-1} b \\\\\n",
    "x &=A^{-1} b\n",
    "\\end{align*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**For example**, let solve these equations: \n",
    "\n",
    "\\begin{eqnarray*} x + 3y + 5z & = & 10 \\\\\n",
    "                   2x + 5y + z & = & 8  \\\\\n",
    "                   2x + 3y + 8z & = & 3\n",
    " \\end{eqnarray*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# In matrix notation: \n",
    "A = np.array([[1., 3., 5.],\n",
    "               [2., 5., 1.],\n",
    "               [2., 3., 8.]])\n",
    "\n",
    "b = np.array([10,8,3])\n",
    "\n",
    "#using a matrix inverse\n",
    "\n",
    "#x = np.linalg.solve(A, b)\n",
    "x = np.dot(np.linalg.inv(A),b)\n",
    "print(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Or use \n",
    "x = np.linalg.solve(A, b)\n",
    "print(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numpy Exercise\n",
    "\n",
    "* Generate a matrix with 10 rows and 50 columns, elements being drawn from normal distribution $\\mathcal{N}(1, 10)$. Specify random seed to make the result reproducible.\n",
    "* Normalize the matrix: subtract from each column its mean and divide by the standard deviation. I suggest np.mean, np.std with axis parameter.\n",
    "* Define function scale which takes a vector of numbers and brings them to the range from 0 to 1:\n",
    "\n",
    "$$ scale(x)=\\frac{x_i - min(x)}{max(x) - min(x)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression with Numpy\n",
    "\n",
    "In regression, we are interested in predicting a scalar-valued target, such as the price of a stock. By linear, we mean that the target must be predicted as a linear function of the inputs. \n",
    "\n",
    "In order to formulate a linear regression, we need to define two things: a model (hypothesis) and a loss function. \n",
    "\n",
    "The model is functions that compute predictions from the inputs given by\n",
    "\n",
    "\n",
    "$$\n",
    "y = \\sum_j w_jx_j + b\n",
    "$$\n",
    "\n",
    "\n",
    "where $w$ is the weights, and $b$ is an intercept term, which we'll call the bias\n",
    "\n",
    "**Loss function** defines how well the model fit the data and thus show how far off the prediction $y$ is from the target $t$ and given as:\n",
    "\n",
    "$$\n",
    "\\mathcal{L(y,t)} = \\frac{1}{2}(y - t)^2\n",
    "$$\n",
    "\n",
    "\n",
    "When we combine our model and loss function, we get an optimization problem, where we are trying to minimize a cost function with respect to the model parameters (i.e. the weights and bias).The cost function is simply the loss, averaged over all the training examples.\n",
    "\n",
    "\\begin{align}\n",
    "\\varepsilon (w_1\\ldots w_D,b) & = \\frac{1}{N}\\sum_{i=1}^N \\mathcal{L}(y^{(i)},t^{(i)})\\\\\n",
    "& = \\frac{1}{2N}\\sum_{i=1}^N (y^{(i)} - t^{(i)})^2\\\\\n",
    "&=\\frac{1}{2N}\\sum_{i=1}^N \\left(\\sum_j w_jx_j^{(i)} + b -t^{(i)} \\right)\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "## Solving the optimization problem\n",
    "\n",
    "\n",
    "We now want to find the choice of model parameters $w_1\\ldots w_D,b$ that minimizes $\\varepsilon (w_1\\ldots w_D,b)$ as given above.There are two methods which we can use: direct solution and gradient descent.\n",
    "\n",
    "Using direct solution it can shown that:\n",
    "$$\n",
    "\\mathbf{w} = (\\mathbf{x^Tx})^{-1}\\mathbf{x^Tt}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us implement this in python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the target function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt  # Plotting library\n",
    "%matplotlib inline\n",
    "np.random.seed(seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define the vector of input samples as x, with 100  values sampled from a uniform distribution between 0 and 1\n",
    "x = np.random.uniform(0, 1, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create the targets t with some gaussian noise\n",
    "noise_variance = 0.2  # Variance of the gaussian noise\n",
    "# Gaussian noise error for each sample in x\n",
    "noise = np.random.randn(100) * noise_variance\n",
    "# Create targets t\n",
    "t = x*2 + noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot the target t versus the input x\n",
    "plt.plot(x, t, 'o', label='t')\n",
    "# Plot the initial line\n",
    "#plt.plot([0, 1], [f(0), f(1)], 'b-', label='f(x)')\n",
    "plt.xlabel('$x$', fontsize=15)\n",
    "plt.ylabel('$t$', fontsize=15)\n",
    "plt.ylim([0,2])\n",
    "plt.title('inputs (x) vs targets (t)')\n",
    "plt.legend(loc=2);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create loss function\n",
    "def loss(x, w, t):\n",
    "    N = x.shape[0]\n",
    "    y = x.dot(w)\n",
    "    loss = (y - t)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w = np.array([2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "loss(x,w,t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#check x shape\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# let us change the shape of x to  be NxD matrix\n",
    "x = x.reshape(len(x), 1)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Run again the loss function\n",
    "loss(x,w,t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cost(x,w, t):\n",
    "    '''\n",
    "    Evaluate the cost function in a vectorized manner for \n",
    "    inputs `x` and targets `t`, at weights `w1`, `w2` and `b`.\n",
    "    '''\n",
    "    N = x.shape[0]\n",
    "    return (loss(x, w,t) **2).sum() / (2.0 * N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_parameter(x, t):\n",
    "    '''\n",
    "    Solve linear regression exactly. (fully vectorized)\n",
    "    \n",
    "    Given `x` - NxD matrix of inputs\n",
    "          `t` - target outputs\n",
    "    Returns the optimal weights as a D-dimensional vector\n",
    "    '''\n",
    "    N, D = np.shape(x)\n",
    "    A = np.matmul(x.T, x)\n",
    "    c = np.dot(x.T, t)\n",
    "    return np.matmul(linalg.inv(A), c)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# let try to run the cost function\n",
    "cost(x,w, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "w_grad = get_parameter(x,t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(x, t, 'o', label='t')\n",
    "plt.plot(x,np.dot(x, w_grad), 'g-', label='torelence $=1e^{-7}$')\n",
    "plt.xlabel('$x$', fontsize=15)\n",
    "plt.ylabel('$t$', fontsize=15)\n",
    "plt.ylim([0,2])\n",
    "plt.title('inputs (x) vs targets (t)')\n",
    "plt.legend(loc=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
